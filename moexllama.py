# -*- coding: utf-8 -*-
"""MoExLLaMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ZJGQnOESTR6IEYnoxCejnVIVBjQoXi0
"""

import torch
from torch import nn
device='cuda' if torch.cuda.is_available() else 'cpu'

from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace, BertPreTokenizer

batch_size=1
seq_len=128
head=8
d_model=512
vocab_size=4000
kv_head = 4
head_dim=64
win_size=16
Main=0

class Embed(nn.Module):
    def __init__(self,vocab_size, dims):
        super().__init__()
        self.dims=dims
        self.embedding=nn.Embedding(vocab_size,dims).to(device)
    def forward(self,x):
        return self.embedding(x) * self.dims**0.5

def rope(dims, seq_len):
    seq=torch.arange(seq_len)
    d=torch.arange(0,dims,2)
    theta=1/10000**(d/dims)
    a=torch.outer(seq, theta)
    y=torch.polar(torch.ones_like(a),a)
    return y.unsqueeze_(0).unsqueeze_(2).to(device)

def rope_apply(x,a):
    x=x.view([*x.shape[:3],head_dim//2, 2])
    x=torch.view_as_complex(x)
    x=x*a
    x=torch.view_as_real(x)
    x=x.view([*x.shape[:3], -1])
    return x.to(device)

class GQA(nn.Module):
    def __init__(self, batch, dim, h, kv_h, seq_len):
        super().__init__()
        assert dim&h==0, 'dim must be divisible by h'
        assert h%kv_h==0, 'h must be divisible by kv_h'
        self.d_h=dim//h
        self.h_div=h//kv_h
        self.kv_h=kv_h
        self.h=h
        self.d_q=nn.Linear(dim, dim).to(device)
        self.d_k=nn.Linear(dim, dim).to(device)
        self.d_v=nn.Linear(dim, dim).to(device)
        self.w_o=nn.Linear(dim, dim).to(device)
        self.k_cache=torch.ones([batch, kv_h, seq_len, self.d_h]).to(device)
        self.v_cache=torch.ones([batch, kv_h, seq_len, self.d_h]).to(device)
    @staticmethod
    def attention(q, k, v, mask=None):
        d_k=128
        mask=torch.tril(torch.triu(torch.ones([seq_len, seq_len], dtype=torch.float, device='cuda'), -win_size+1))
        attn=q@k.transpose(-2, -1)/d_k**0.5
        if mask is not None:
          attn=attn.masked_fill(mask==0, -1e9)
        attn=torch.softmax(attn, dim=-1)
        out=attn@v
        return out.to(device), attn
    @staticmethod
    def flush(k,v):
        k=torch.ones([batch_size, kv_head, seq_len, 64]).to(device)
        v=torch.ones([batch_size, kv_head, seq_len, 64]).to(device)
        return k,v
    def forward(self, x, a):
        q_=self.d_q(x).view(*x.shape[:2], self.h, self.d_h)
        k_=self.d_k(x).view(x.shape[0], -1, self.h, self.d_h)
        v_=self.d_v(x).view(x.shape[0], -1, self.h, self.d_h).transpose(1,2)
        a_=a
        q=rope_apply(q_, a_).transpose(1,2)
        k=rope_apply(k_, a_).transpose(1,2)
        v=v_
        y=k.shape
        y, attn = GQA.attention(q, k, v)
        y=y.transpose(1,2).contiguous()
        y=y.view(*y.shape[:2],-1)
        y=self.w_o(y)
        self.k_cache, self.v_cache=GQA.flush(self.k_cache, self.v_cache)
        return y.to(device), attn

class RMSnorm(nn.Module):
    def __init__(self, dim=None):
        super().__init__()
        self.alpha=nn.Parameter(torch.ones(512))
    def forward(self, x):
        x=(x/torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True)))*self.alpha
        return x.to(device)

class FF(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.d1=dim
        self.dff=int((2/3)*4*dim)
        self.l1=nn.Linear(dim, self.dff, bias=False).to(device)
        self.l2=nn.Linear(dim, self.dff, bias=False).to(device)
        self.l3=nn.Linear(self.dff, dim, bias=False).to(device)
    def forward(self, x):
        x=nn.functional.silu(self.l1(x))*self.l2(x)
        x=self.l3(x)
        return x.to(device)

class gate(nn.Module):
    def __init__(self, dim, num_exp=8):
        super().__init__()
        self.Wg=nn.Linear(dim, num_exp).to(device)
        self.Wn=nn.Linear(dim, num_exp).to(device)
        self.d=dim
        self.ne=num_exp
    def forward(self, x, train=False):
        a=self.Wg(x)
        b=self.Wn(x)
        if train:
            return a + torch.nn.functional.soft(b)
        else:
            return a + torch.randn([self.ne], device='cuda') * torch.nn.functional.softplus(b)

class Expert(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim1=dim
        self.dim2=int((2/3)*4*dim)
        self.lst=nn.Sequential(nn.Linear(self.dim1, self.dim2),
                               nn.SiLU(),
                               nn.Linear(self.dim2, self.dim1)).to(device)
    def forward(self, x):
        return self.lst(x)

class FF(nn.Module):
    def __init__(self, gate, exp):
        super().__init__()
        self.gate=gate
        self.exp=exp
    def forward(self, x):
        Z,S,D = x.shape
        #D=H*D
        y=self.gate(x)
        a,b=torch.topk(y,8, dim=2, sorted=False)
        b=b+1
        b[b>2]=0
        z=a*b
        z[z==0] = float('-inf')
        z=torch.softmax(z, -1).unsqueeze(-2)
        b=torch.where(b!=0,1,0)
        b=b * torch.arange(1,9).view(1,8).to(device)
        b=b.view(-1,8)
        z=z.view(-1, 8)
        a=a.view(-1,8)
        y=x.view(-1, 512)
        out=torch.zeros_like(y)
        for i, exp in enumerate(self.exp):
            mask=(b==i+1).any(dim=-1)
            if mask.any():
                out[mask] = out[mask] + z[mask, i].unsqueeze(-1) * exp(y[mask])
        return out.view(Z,S,D)

class Decoder(nn.Module):
    def __init__(self, norm:RMSnorm, gqa:GQA, ff:FF):
        super().__init__()
        self.norm=norm
        self.gqa=gqa
        self.ff=ff
    def forward(self,x, a):
        y, attn=self.gqa(self.norm(x), a)
        y=y.to(device)
        x=x+self.norm(y).to(device)
        y=self.ff(self.norm(x)).to(device)
        x=x+y
        return x.to(device), attn

class block(nn.Module):
    def __init__(self, dec, norm:RMSnorm):
        super().__init__()
        self.dec=dec.to(device)
        self.norm=norm.to(device)
    def forward(self,x,a):
        for i,j in enumerate(self.dec):
            x, attn=j(x, a)
            x=self.norm(x)
        return x.to(device), attn

class project(nn.Module):
    def __init__(self, dim, vocab_size):
        super().__init__()
        self.dim=dim
        self.l1=nn.Linear(dim, vocab_size)
    def forward(self, x):
        x=torch.log_softmax(self.l1(x), dim=-1).to(device)
        return x.to(device)

class Transformer(nn.Module):
    def __init__(self, emb, dec, proj):
        super().__init__()
        self.emb=emb
        self.dec=dec
        self.proj=proj
    def decode(self,x, a):
        x=self.emb(x)
        x, attn=self.dec(x, a)
        return x, attn
    def project(self, x):
        x=self.proj(x)
        return x

def llama_(batch_size, vocab_size, seq_len, d_model, head, kv_head):
    N=8
    embedding=Embed(vocab_size, d_model)
    decoder=nn.ModuleList()
    for _ in range(N):
        norm=RMSnorm()
        gqa=GQA(batch_size, d_model, head, kv_head, seq_len)
        G=gate(512)
        m=[Expert(512) for _ in range(8)]
        ff=FF(G, m)
        dec=Decoder(norm, gqa, ff)
        decoder.append(dec)
    decblock=block(decoder,RMSnorm())
    proj=project(d_model, vocab_size)
    transformer=Transformer(embedding, decblock, proj)
    return transformer.to(device)

h=llama_(32,4000,128,512,8,4)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
def save_fig(attn, t, i):
  plt.figure(figsize=(19, 19))
  plasma = plt.cm.plasma
  plasma.set_bad(color='white')
  masked_attn = np.ma.masked_where(attn == 0, attn)
  plt.imshow(masked_attn, cmap=plasma)
  x_labels = t
  y_labels = t
  plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels, rotation=90)
  plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels)
  plt.savefig(f'/content/snaps/masked_attention_plot {i}.png', bbox_inches='tight')

def get_all_sentences(ds):
    for item in ds:
        yield item['translation'][lang]
def build_tok(ds):
    tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
    tokenizer.pre_tokenizer = BertPreTokenizer()
    trainer = WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)
    tokenizer.train_from_iterator(ds, trainer=trainer)
    return tokenizer
def get_ds():
    ds_raw = a
    src_tok=build_tok(ds_raw)
    return src_tok.encode(' '.join(ds_raw)).ids, src_tok

f=open('data.txt','r')
a=f.read()
a=a.split(' ')
src_tok, tok=get_ds()
t=[]
s=[]
for i in range(320):
  src=src_tok[128*i:128*(i+1)]
  tgt=src_tok[128*i+1:128*(i+1)+1]
  s.append(src)
  t.append(tgt)
t=torch.tensor(t).to(device)
s=torch.tensor(s).to(device)

a=rope(64,128)
print(a.shape)

import time
crit=nn.CrossEntropyLoss()
optimizer=torch.optim.AdamW(h.parameters(), lr=0.0001)
a=rope(64,128)
for j in range(400):
  st=time.time()
  print(f'Epoch: {j}')
  for i in range(10):
    y, attn=h.decode(s[32*i:32*(i+1),:],a)
    y=h.project(y)
    loss=crit(y.view(-1,4000),t[32*i:32*(i+1),:].view(-1))
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
  y, attn=h.decode(s[32*9:32*(9+1),:],a)
  y=h.project(y)
  _, idx=torch.max(y, dim=-1)
  attn=attn[0,0,:72,:72].cpu().detach().numpy()
  b=tok.decode(list(idx[0])).split()
  if len(b)<120:
    b.extend(['pad' for i in range(120-len(b))])
  save_fig(attn, b[:72], j)
  print(tok.decode(list(idx[0])))
  print(loss.item())
  sp=time.time()
  print(sp-st)